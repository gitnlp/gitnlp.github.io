<!DOCTYPE html>
<html lang="en">

<head>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-71156606-1');
    </script>
    <meta charset="utf-8" />
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Advancing AGI for humanity | Foundation of AGI</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <link rel="stylesheet" type="text/css" href="./assets/css/main.css" />

    <script type="text/javascript">document.documentElement.className = 'js';</script>
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:image:width" content="3200" />
    <meta property="og:image:height" content="1800" />

    <meta name="generator" content="Ghost 5.12" />
    <style id="gh-members-styles">
        .gh-post-upgrade-cta-content,
        .gh-post-upgrade-cta {
            display: flex;
            flex-direction: column;
            align-items: center;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            text-align: center;
            width: 100%;
            color: #ffffff;
            font-size: 16px;
        }

        .gh-post-upgrade-cta-content {
            border-radius: 8px;
            padding: 40px 4vw;
        }

        .gh-post-upgrade-cta h2 {
            color: #ffffff;
            font-size: 28px;
            letter-spacing: -0.2px;
            margin: 0;
            padding: 0;
        }

        .gh-post-upgrade-cta p {
            margin: 20px 0 0;
            padding: 0;
        }

        .gh-post-upgrade-cta small {
            font-size: 16px;
            letter-spacing: -0.2px;
        }

        .gh-post-upgrade-cta a {
            color: #ffffff;
            cursor: pointer;
            font-weight: 500;
            box-shadow: none;
            text-decoration: underline;
        }

        .gh-post-upgrade-cta a:hover {
            color: #ffffff;
            opacity: 0.8;
            box-shadow: none;
            text-decoration: underline;
        }

        .gh-post-upgrade-cta a.gh-btn {
            display: block;
            background: #ffffff;
            text-decoration: none;
            margin: 28px 0 0;
            padding: 8px 18px;
            border-radius: 4px;
            font-size: 16px;
            font-weight: 600;
        }

        .gh-post-upgrade-cta a.gh-btn:hover {
            opacity: 0.92;
        }
    </style>
    <script defer src="./assets/js/cards.min.js"></script>
    <style>
        :root {
            --ghost-accent-color: #15171A;
        }
        tag {
            /* color: -webkit-link; */
            font-style: italic;
            font-size: 16px;
            color: black;
            /* color: white;
            background-color: #264653!important;
            font-weight: 600;
            padding: 0px 3px 0 3px;
            margin-right: 5px; */
        }
        /* tag.conf {
            color: white;
            background-color: #2a9d8f!important;
            font-weight: 600;
            padding: 0px 3px 0 3px;
        } */
        
    </style>
    <link rel="stylesheet" type="text/css" href="./assets/css/cards.min.css">
</head>

<body>

    <header>
        <nav class="nav container" data-url="/blog/">
            <div class="nav-row row align-items-center">
                <div class="d-none d-sm-block col-sm nav-symbol-wrap">
                    <a href="./index.html"> Home </a>
                </div>
                <div class="col col-sm-auto">
                    <ul class="d-flex flex-row align-items-center justify-content-between small-caps">
                        <div class="d-sm-none nav-symbol-wrap">
                        </div>
                        
                        <!--li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="overview.html"
                                data-slug="research">Overview</a></li-->

                        <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link active" href="research.html"
                                data-slug="research">Research</a></li>

                        <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="blog.html"
                                data-slug="blog">Blog</a></li>

                        <li class="ml-sm-1.75" style="margin-top:0.5px"><a class="nav-link" href="about.html"
                                data-slug="about">About</a></li>
                    </ul>
                </div>
            </div>
        </nav>

    </header>
    
   
    <div class="container mt-4">
        <h2 class="mb-2">Publication</h2>
        <p style="padding-bottom:20px"><a href="overview.html"><font color="blue">Highlights</font></a></p>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2023</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2306.08543>Knowledge Distillation of Large Language Models</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2306.08543 class="color-fg-50">
                               We propose MiniLLM that distills smaller language models from generative larger language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution.
                                <br>
                                <time datetime="2023-06-14">June 14, 2023</time>
                            </a>
                            <br>
                            <tag> #MiniLLM </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2023</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2306.07174>Augmenting Language Models with Long-Term Memory</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2306.07174 class="color-fg-50">
                               We propose a framework, Language Models Augmented with Long-Term Memory (LongMem), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness.
                                <br>
                                <time datetime="2023-06-12">June 12, 2023</time>
                            </a>
                            <br>
                            <tag> #Long-term Memory </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2023</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2305.10855>TextDiffuser: Diffusion Models as Text Painters</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2305.10855 class="color-fg-50">
                               Diffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce TextDiffuser, focusing on generating images with visually appealing text that is coherent with backgrounds.
                                <br>
                                <time datetime="2023-05-11">May 16, 2023</time>
                            </a>
                            <br>
                            <tag> #TextDiffuser </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2023</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2305.09137>Pre-Training to Learn in Context</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2305.09137 class="color-fg-50">
                               <tag>ACL'23 </tag>We propose PICL (Pre-training for In-Context Learning), a framework to enhance the language models' in-context learning ability by pre-training the model on a large collection of "intrinsic tasks" in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models.
                                <br>
                                <time datetime="2023-05-11">May 16, 2023</time>
                            </a>
                            <br>
                            <tag> </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2023</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href=https://arxiv.org/abs/2305.07004>Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting</a></h5>
                        <div>
                            <a href=https://arxiv.org/abs/2305.07004 class="color-fg-50">
                               We introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages.
                                <br>
                                <time datetime="2023-05-11">May 11, 2023</time>
                            </a>
                            <br>
                            <tag> #LMOps #Cross-Lingual-Thought </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2023</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2304.04487">Inference with Reference: Lossless Acceleration of Large Language Models</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2304.04487" class="color-fg-50">
                               We propose LLMA, an LLM accelerator to losslessly speed up Large Language Model (LLM) inference with references. LLMA is motivated by the observation that there are abundant identical text spans between the decoding result by an LLM and the reference that is available in many real world scenarios (e.g., retrieved documents). The improved computational parallelism allows LLMA to achieve over 2x speed-up for LLMs with identical generation results as greedy decoding in many practical generation scenarios.
                                <br>
                                <time datetime="2023-04-10">April 10, 2023</time>
                            </a>
                            <br>
                            <tag> #LMOps LLM Accelerator </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2023</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2302.14045">Language Is Not All You Need: Aligning Perception with Language Models</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2302.14045" class="color-fg-50">
                               A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot).
                                <br>
                                <time datetime="2023-02-27">February 27, 2023</time>
                            </a>
                            <br>
                            <tag> #AGI #MLLM Kosmos-1 </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2023</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2303.03926">Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2303.03926" class="color-fg-50">
                               We extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks.
                                <br>
                                <time datetime="2023-03-07">March 7, 2023</time>
                            </a>
                            <br>
                            <tag> #Speech VALL-E </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2023">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2023</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2301.02111">Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2301.02111" class="color-fg-50">
                               We train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt.
                                <br>
                                <time datetime="2023-01-06">January 6, 2023</time>
                            </a>
                            <br>
                            <tag> #Speech VALL-E </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.10554">A Length-Extrapolatable Transformer</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.10554" class="color-fg-50">
                               <tag>ACL'23 </tag>In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences. Specifically, we introduce a relative position embedding to explicitly maximize attention resolution. Moreover, we use blockwise causal attention during inference for better resolution.
                                <br>
                                <time datetime="2022-12-20">December 20, 2022</time>
                            </a>
                            <br>
                            <tag> #TorchScale Length-Extrapolatable Transformers </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.10559">Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta Optimizers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.10559" class="color-fg-50">
                               <tag>ACL'23 </tag>This paper explains language models as meta optimizers and understands ICL as a kind of implicit finetuning. Theoretically, we figure out that the Transformer attention has a dual form of gradient descent based optimization. 
                                <br>
                                <time datetime="2022-12-20">December 20, 2022</time>
                            </a>
                            <br>
                            <tag> #Fundamentals In-Context Learning </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.09611">Optimizing Prompts for Text-to-Image Generation</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.09611" class="color-fg-50">
                               We propose prompt adaptation, a general framework based on reinforcement learning that automatically adapts original user input to model-preferred prompts. 
                                <br>
                                <time datetime="2022-12-19">December 19, 2022</time>
                            </a>
                            <br>
                            <tag> #LMOps #Prompt_Intelligence Promptist </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.09058">BEATs: Audio Pre-Training with Acoustic Tokenizers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.09058" class="color-fg-50">
                               <tag>ICML'23 </tag>We propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. 
                                <br>
                                <time datetime="2022-12-18">December 18, 2022</time>
                            </a>
                            <br>
                            <tag> #Speech </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.06713">Structured Prompting: Scaling In-Context Learning to 1,000 Examples</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.06713" class="color-fg-50">
                               Conventional in-context learning is usually restricted by length constraints, rendering it ineffective to absorb supervision from a large number of examples. In order to go beyond few shots, we introduce structured prompting that breaks the length limit and scales in-context learning to thousands of examples. 
                                <br>
                                <time datetime="2022-12-13">December 13, 2022</time>
                            </a>
                            <br>
                            <tag> #LMOps #Prompt_Intelligence Structured Prompting </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.00616">Extensible Prompts for Language Models</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.00616" class="color-fg-50">
                               We propose eXtensible Prompt (X-Prompt) for prompting a large language model (LLM) beyond natural language (NL). X-Prompt instructs an LLM with not only NL but also an extensible vocabulary of imaginary words that are introduced to help represent what NL words hardly describe, allowing a prompt to be more descriptive. 
                                <br>
                                <time datetime="2022-12-01">December 1, 2022</time>
                            </a>
                            <br>
                            <tag> #LMOps #Prompt_Intelligence X-Prompt </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2212.03533">Text Embeddings by Weakly-Supervised Contrastive Pre-training</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2212.03533" class="color-fg-50">
                               This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs).
                                <br>
                                <time datetime="2022-12-07">December 7, 2022</time>
                            </a>
                            <br>
                            <tag> E5 </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2211.13184">TorchScale: Transformers at Scale</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2211.13184" class="color-fg-50">
                               We present TorchScale, an open-source toolkit that allows researchers and developers to scale up Transformers efficiently and effectively. TorchScale has the implementation of several modeling techniques, which can improve modeling generality and capability, as well as training stability and efficiency. 
                                <br>
                                <time datetime="2022-11-23">November 23, 2022</time>
                            </a>
                            <br>
                            <tag> #TorchScale #Foundation_Transformers Magneto DeepNet X-MoE </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2210.06423">Magneto: A Foundation Transformer</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2208.10442" class="color-fg-50">
                                <tag>ICML'23 </tag>We call for the development of Foundation Transformer for true general-purpose modeling, which serves as a go-to architecture for various tasks and modalities with guaranteed training stability. In this work, we introduce a Transformer variant, named Magneto, to fulfill the goal. Specifically, we propose Sub-LayerNorm for good expressivity, and the initialization strategy theoretically derived from DeepNet for stable scaling up. Extensive experiments demonstrate its superior performance and better stability than the de facto Transformer variants designed for various applications, including language modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e., BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).
                                <br>
                                <time datetime="2022-10-12">October 12, 2022</time>
                            </a>
                            <br>
                            <tag> #TorchScale #Foundation_Transformers Magneto DeepNet </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2022</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2208.10442">Image as a Foreign Language: BEiT Pretraining
                                for All Vision and Vision-Language Tasks</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2208.10442" class="color-fg-50">
                                <tag>CVPR'23 </tag>We present BEiT-3, a general-purpose multimodal foundation model which achieves SOTA across 10+ major vision and vision-language benchmarks. BEiT-3 = Multiway Transformers + Masked Data Modeling + Scaling Up.
                                <br>
                                <time datetime="2022-08-31">August 22, 2022</time>
                            </a>
                            <br>
                            <tag> #multimodal #vision BEiT-3 </tag> 
                        </div>
                    </div>
                </div>
                <!-- <div class="col-12 col-md-4">
                    <img class="position-absolute trbl-0 js-lazy js-lazy-loaded"
                                            alt="BEIT-3"
                                            src="./assets/img/beitv3.png"> -->
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2206.06336">Language Models are General-Purpose Interfaces</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2206.06336" class="color-fg-50">
                                We propose to use language models as a general-purpose interface to various foundation models. 
                                A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer.
                                <br>
                                <time datetime="2022-06-13">June 13, 2022</time>
                            </a>
                            <br>
                            <tag> #multimodal #general-purpose-interface MetaLM </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2208.06366">BEiT v2: Masked Image Modeling with
                                Vector-Quantized Visual Tokenizers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2208.06366" class="color-fg-50">
                                Use a semantic-rich visual tokenizer as the reconstruction target for masked prediction,
                                providing a systematic way to promote MIM from pixel-level to semantic-level.
                                <br>
                                <time datetime="2022-08-31">August 12, 2022</time>
                            </a>
                            <br>
                            <tag> #vision BEiT(-2) </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2210.10615">A Unified View of Masked Image Modeling</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2210.10615" class="color-fg-50">
                                We propose a unified view of masked image modeling after revisiting existing methods. Under the unified view, we introduce a simple yet effective method, 
                                termed as MaskDistill, which reconstructs normalized semantic features from teacher models at the masked positions, conditioning on corrupted input images.
                                <br>
                                <time datetime="2022-10-19">October 19, 2022</time>
                            </a>
                            <br>
                            <tag> #vision MIM </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2209.15329">SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2209.15329" class="color-fg-50">
                               We propose a cross-modal Speech and Language Model (SpeechLM) to explicitly align speech and text pre-training with a pre-defined unified discrete representation.
                                Leveraging only 10K text sentences, our SpeechLM gets a 16\% relative WER reduction over the best base model performance (from 6.8 to 5.7) on the public LibriSpeech ASR benchmark.
                                <br>
                                <time datetime="2022-09-30">September 30, 2022</time>
                            </a>
                            <br>
                            <tag> #speech SpeechLM </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2207.02578">SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2207.02578" class="color-fg-50">
                               <tag>ACL'23 </tag>We propose a simple yet effective pre-training method for dense passage retrieval.
                                <br>
                                <time datetime="2022-07-06">July 6, 2022</time>
                            </a>
                            <br>
                            <tag> #language SimLM </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2205.10350">Lossless Acceleration for Seq2seq Generation with Aggressive Decoding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2205.10350" class="color-fg-50">
                               We study lossless acceleration for seq2seq generation with a novel decoding algorithm -- Aggressive Decoding.
                                <br>
                                <time datetime="2022-05-20">May 20, 2022</time>
                            </a>
                            <br>
                            <tag> #language </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2204.09179">On the Representation Collapse of Sparse Mixture of Experts</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2204.09179" class="color-fg-50">
                                 <tag>NeurIPS'22 </tag>Our method alleviates the representation collapse issue and achieves more consistent routing than the baseline mixture-of-experts methods.
                                <br>
                                <time datetime="2022-08-31">April 20, 2022</time>
                            </a>
                            <br>
                            <tag> #TorchScale #MoE X-MoE </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2204.08396">StableMoE: Stable Routing Strategy for Mixture of Experts</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2204.08396" class="color-fg-50">
                                <tag>ACL'22 </tag>We propose StableMoE with two training stages to address the routing fluctuation problem.
                                <br>
                                <time datetime="2022-08-31">April 18, 2022</time>
                            </a>
                            <br> <tag> #language #MoE </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2204.08387">LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2204.08387" class="color-fg-50">
                                <tag>ACM MM'22 </tag>Pre-train multimodal Transformers for Document AI with unified text and image masking.
                                <br>
                                <time datetime="2022-08-31">April 18, 2022</time>
                            </a>
                            <br> <tag> #multimodal LayoutLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2203.02378">DiT: Self-supervised Pre-training for Document Image Transformer</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2203.02378" class="color-fg-50">
                                <tag> ACM MM'22 </tag>A self-supervised pre-trained DiT ransformer model using large-scale unlabeled text images for Document AI tasks.
                                <br>
                                <time datetime="2022-08-31">March 4, 2022</time>
                            </a>
                            <br><tag> #vision DiT BEiT </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2203.00555">DeepNet: Scaling Transformers to 1,000 Layers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2203.00555" class="color-fg-50">
                                We introduce a new normalization function (DeepNorm) to modify the residual connection in Transformer, accompanying with theoretically derived initialization.
                                <br>
                                <time datetime="2022-08-31">March 1, 2022</time>
                            </a>
                            <br>
                            <tag> #TorchScale DeepNet </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2202.07959">EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2202.07959" class="color-fg-50">
                                <tag>EMNLP'22</tag> We propose EdgeFormer -- a parameter-efficient Transformer of the encoder-decoder architecture for on-device seq2seq generation, which is customized under strict computation and memory constraints.
                                <br>
                                <time datetime="2022-02-16">February 16, 2022</time>
                            </a>
                            <br>
                            <tag> #language </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2111.02358">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2111.02358" class="color-fg-50">
                                <tag>NeurIPS'22</tag> We present a unified Vision-Language pretrained Model (VLMo) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network.. 
                                <br>
                                <time datetime="2021-11-03">November 03, 2021</time>
                            </a>
                            <br>
                            <tag> #multimodal VLMo Multiway Transformers </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2110.13900">WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2110.13900" class="color-fg-50">
                                <tag>JSTSP'22</tag> Pre-training for full-stack speech processing tasks. WavLM jointly learns masked speech prediction and denoising in pre-training, and it achieves state-of-the-art performance on the SUPERB benchmark. 
                                <br>
                                <time datetime="2022-08-31">October 26, 2021</time>
                            </a>
                            <br>
                            <tag> #speech WavLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2110.08518">MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2110.08518" class="color-fg-50">
                                <tag>ACL'22 </tag>We propose MarkupLM for document understanding tasks with markup languages as the backbone, such as HTML/XML-based documents, where text and markup information is jointly pre-trained. 
                                <br>
                                <time datetime="2021-10-16">October 16, 2021</time>
                            </a>
                            <br>
                            <tag> #language MarkupLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2110.07205">SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2110.07205" class="color-fg-50">
                                <tag>ACL'22 </tag>We propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. 
                                <br>
                                <time datetime="2021-10-14">October 14, 2021</time>
                            </a>
                            <br>
                            <tag> #speech SpeechT5 </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2109.07306">Allocating Large Vocabulary Capacity for Cross-lingual Language Model Pre-training</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2109.07306" class="color-fg-50">
                                <tag> EMNLP'21 </tag>We propose an algorithm VoCap to determine the desired vocabulary capacity of each language.
                                <br>
                                <time datetime="2022-09-15">September 15, 2021</time>
                            </a>
                            <br> <tag> #multilingual XLM-E </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2106.16138">XLM-E: Cross-lingual Language Model Pre-training via ELECTRA </a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2106.16138" class="color-fg-50">
                                <tag> ACL'22 </tag>ELECTRA-style tasks to cross-lingual language model pre-training.
                                <br>
                                <time datetime="2022-08-31">June 30, 2021</time>
                            </a>
                            <br> <tag> #multilingual XLM-E </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2106.13474">Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2106.13474" class="color-fg-50">
                                <tag> ACL'21 </tag>We present a general approach to developing small, fast and effective pre-trained models for specific domains.
                                <br>
                                <time datetime="2021-06-25">June 25, 2021</time>
                            </a>
                            <br> <tag> #language AdaLM incr_bpe </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2106.08254">BEiT: BERT Pre-Training of Image Transformers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2106.08254" class="color-fg-50">
                                <tag> ICLR'22 (<font color="blue">Oral</font>) </tag>We propose Maskded Image Modeling to pretrain vision Transformers.
                                <br>
                                <time datetime="2022-08-31">June 15, 2021</time>
                            </a> 
                            <br>
                            <tag> #vision BEiT </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2021</div>
                    <div class="mb-1.5">
                        
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2106.08226">Consistency Regularization for Cross-Lingual Fine-Tuning</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2106.08226" class="color-fg-50">
                                <tag> ACL'21 (<font color="blue">Oral</font>) </tag>We propose to improve cross-lingual fine-tuning with consistency regularization. Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation.
                                <br>
                                <time datetime="2022-08-31">June 15, 2021</time>
                            </a> 
                            <br>
                            <tag> #multilingual xTune </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2104.08836">LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2104.08836" class="color-fg-50">
                                <tag> ACL'22 </tag>We present a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding.
                                <br>
                                <time datetime="2022-08-31">April 18, 2021</time>
                            </a>
                            <br> <tag> #multimodal #multilingual Layout(X)LM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2104.08692">mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs </a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2104.08692" class="color-fg-50">
                                <tag> EMNLP'21 </tag>Improve multilingual text-to-text transfer Transformer with translation pairs.
                                <br>
                                <time datetime="2022-08-31">April 18, 2021</time>
                            </a>
                            <br> <tag> #multilingual DeltaLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2104.08696">Knowledge Neurons in Pretrained Transformers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2104.08696" class="color-fg-50">
                                <tag> ACL'22 </tag>A preliminary study on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons.
                                <br>
                                <time datetime="2022-08-31">March 10, 2021</time>
                            </a>
                            <br>
                            <tag> #language </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2021">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2101.07597">UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2101.07597" class="color-fg-50">
                                <tag> ICML'21 </tag>we propose a unified pre-training approach to learning speech representations with both unlabeled and labeled data.
                                <br>
                                <time datetime="2021-01-19">January 19, 2021</time>
                            </a>
                            <br>
                            <tag> #speech </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2020</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2012.15828">MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2012.15828" class="color-fg-50">
                                <tag> ACL'21 </tag>Employ multi-head self-attention relations (scaled dot-product between the pairs of query, key, and value vectors within each self-attention module) to train the student model.
                                <br>
                                <time datetime="2022-08-31">December 31, 2020</time>
                            </a>
                            <br><tag> #language MiniLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
          <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2020</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2012.14740">LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2012.14740" class="color-fg-50">
                                <tag> ACL'21 </tag>We propose a new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework.
                                <br>
                                <time datetime="2020-12-29">December 29, 2020</time>
                            </a>
                            <br><tag> #multimodal LayoutLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2007.07834">InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training </a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2007.07834" class="color-fg-50">
                                <tag> NAACL'21 </tag>An information-theoretic framework that formulates cross-lingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. 
                                <br>
                                <time datetime="2022-08-31">July 15, 2020</time>
                            </a>
                            <br><tag> #multilingual InfoXLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2020</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2004.11207">Self-Attention Attribution: Interpreting Information Interactions Inside Transformer</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2004.11207" class="color-fg-50">
                                <tag> AAAI'21 (<font color="blue">Best Paper Runner Up</font>) </tag>A self-attention attribution method to interpret the information interactions inside Transformer..
                                <br>
                                <time datetime="2022-08-31">April 23, 2020</time>
                            </a>
                            <br><tag> #language </tag>
                            
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="post-card-full medium-xsmall-copy" data-year="2022">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2002.12804">UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training </a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2104.08692" class="color-fg-50">
                                <tag> ICLM'20 </tag>Pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model. 
                                <br>
                                <time datetime="2022-08-31">February 28, 2020</time></a>
                            <br>
                            <tag> #language UniLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>


        <div class="post-card-full medium-xsmall-copy" data-year="2020">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/2002.10957">MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/2002.10957" class="color-fg-50">
                                <tag> NeurIPS'20 </tag>A simple and effective approach to compress large Transformer based pre-trained models, termed as deep self-attention distillation.
                                <br>
                                <time datetime="2022-08-31">February 25, 2020</time>
                            </a>
                            <br> <tag> #language MiniLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>     
        

        <div class="post-card-full medium-xsmall-copy" data-year="2019">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="post-card-full-hide color-fg-50 mb-1.5">2019</div>
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/1912.13318">LayoutLM: Pre-training of Text and Layout for Document Image Understanding</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/1912.13318" class="color-fg-50">
                                <tag> KDD'20 </tag>Jointly model interactions between text and layout information across scanned document images.
                                <br>
                                <time datetime="2022-08-31">December 31, 2019</time>
                            </a>
                            <br> <tag> #multimodal LayoutLM </tag> 
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="post-card-full medium-xsmall-copy" data-year="2019">
            <div class="row">
                <div class="col-12 col-md-8">
                    <div class="mb-1.5">
                        <h5 class="medium-xsmall-copy balance-text mb-1/12"><a
                                href="https://arxiv.org/abs/1905.03197">Unified Language Model Pre-training for Natural Language Understanding and Generation</a></h5>
                        <div>
                            <a href="https://arxiv.org/abs/1905.03197" class="color-fg-50">
                                <tag> NeurIPS'19 </tag>A new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. 
                                <br>
                                <time datetime="2022-08-31">May 8, 2019</time>
                            </a>
                            <br><tag> #language UniLM </tag>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <footer class="footer container medium-xsmall-copy line-height-1.6">

        <div class="row align-items-center mb-0.125">
            <div class="col-12 col-md mb-0.5">
                <a class="fade" style="margin-top:1px" href="/">&copy; 2022</a>
            </div>
        </div>
    </footer>

    <script type="text/javascript" src="./assets/js/main.min.js"></script>


</body>

</html>
